{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbe44cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import fnmatch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from  numpy import exp,absolute\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.neural_network import MLPClassifier as mlp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, TensorDataset, Subset\n",
    "import pandas as pd\n",
    "from torchmetrics.functional.classification import multiclass_confusion_matrix as mcm\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import accuracy_score,f1_score,precision_score ,recall_score \n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcf3b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper params\n",
    "dataset_path = './data/binary_dataset'\n",
    "\n",
    "bs = 64\n",
    "test_size = 0.15  # Test set size (20%)\n",
    "val_size = 0.2   # Validation set size (25%)\n",
    "num_epoch = 60\n",
    "patience = 20\n",
    "classes = ['IRRELEVANT', 'RELEVANT'] #arranged in order of their placement in the folder\n",
    "num_classes = len(classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c771cd8e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_denoise(image):\n",
    "    # Denoising transformation (replace with your denoising algorithm)\n",
    "    # Example: Apply Gaussian blur with a kernel size of 3\n",
    "    denoised_image = transforms.functional.gaussian_blur(image, kernel_size=3,sigma=1.5)\n",
    "    return denoised_image\n",
    "\n",
    "def transform_enhance(image):\n",
    "    # Image enhancement transformation (replace with your enhancement algorithm)\n",
    "    # Example: Apply contrast enhancement\n",
    "    enhanced_image = transforms.functional.adjust_contrast(image, contrast_factor=1.5)\n",
    "    return enhanced_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "338e5b0e-59b4-4bdc-96dc-772b585bb303",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetFromSubset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a78336-04ee-4ce6-b01b-cce532f30279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample(dataset, r=0.):\n",
    "    # Get the labels for the dataset\n",
    "    labels = np.asarray(dataset.dataset.imgs)[:, 1][dataset.indices]\n",
    "\n",
    "    # Count the number of samples per class\n",
    "    class_counts = defaultdict(int)\n",
    "    for label in labels:\n",
    "        class_counts[label] += 1\n",
    "\n",
    "    # Calculate m\n",
    "    m = min(class_counts.values())\n",
    "\n",
    "    # Calculate the threshold for each class\n",
    "    threshold = m + r * m\n",
    "\n",
    "    # Create a new subset of the dataset\n",
    "    indices = []\n",
    "    new_class_counts = defaultdict(int)\n",
    "    for i in range(len(dataset)):\n",
    "        label = labels[i]\n",
    "        if new_class_counts[label] < threshold:\n",
    "            indices.append(i)\n",
    "            new_class_counts[label] += 1\n",
    "\n",
    "    return Subset(dataset, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40607c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path,val_split,test_split):\n",
    "    transform_train = transforms.Compose([\n",
    "                                        transforms.Resize((256, 256)),\n",
    "                                        transforms.Lambda(lambda x: transform_denoise(x)),\n",
    "                                        transforms.Lambda(lambda x: transform_enhance(x)),\n",
    "                                        transforms.RandomHorizontalFlip(),\n",
    "                                        transforms.RandomRotation(degrees=10),\n",
    "                                        transforms.RandomCrop(size=256),\n",
    "                                        transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                        ])\n",
    "    transform_test = transforms.Compose([\n",
    "                                        transforms.Resize((256, 256)),\n",
    "                                        transforms.Lambda(lambda x: transform_denoise(x)),\n",
    "                                        transforms.Lambda(lambda x: transform_enhance(x)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                        ])\n",
    "\n",
    "\n",
    "    dataset = datasets.ImageFolder(path)    \n",
    "    trainset,valset,testset = stratified(dataset, r=.0)\n",
    "\n",
    "    trainset = DatasetFromSubset(trainset, transform=transform_train)\n",
    "    valset = DatasetFromSubset(valset, transform=transform_test)\n",
    "    testset = DatasetFromSubset(testset, transform=transform_test)\n",
    "    \n",
    "    print(\"train :\",len(trainset),\"val :\",len(valset),\"test :\",len(testset))\n",
    "    \n",
    "    return trainset,valset,testset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b40e7a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_indices(indices, images, name):\n",
    "    file_list = np.asarray(images)[indices]\n",
    "\n",
    "    with open(Path(dataset_path) / (name + '.txt'), 'w') as f:\n",
    "        f.writelines(file_list)\n",
    "\n",
    "\n",
    "def stratified(dataset, r=0.):\n",
    "    # Get the labels and targets from the dataset\n",
    "    labels = np.asarray(dataset.imgs)[:, 1]\n",
    "\n",
    "    stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n",
    "    train_val_indices, test_indices = next(stratified_split.split(np.zeros(len(labels)), labels))\n",
    "\n",
    "    train_val_dataset = torch.utils.data.Subset(dataset, train_val_indices)\n",
    "    testset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "    # write down the splits\n",
    "    imgs = [img[0] + '\\n' for img in dataset.imgs]\n",
    "    write_indices(train_val_indices, imgs, \"train_images\")\n",
    "    write_indices(test_indices, imgs, \"test_images\")\n",
    "    with open(Path(dataset_path) / \"all_images.txt\", \"w\") as f:\n",
    "        f.writelines(imgs)\n",
    "\n",
    "    # balance the train set\n",
    "    train_val_dataset = subsample(train_val_dataset, r=r)\n",
    "    \n",
    "    # Further split the train-val dataset into train and validation sets\n",
    "    stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=42)\n",
    "    train_indices, val_indices = next(stratified_split.split(train_val_dataset, labels[train_val_indices][train_val_dataset.indices]))\n",
    "\n",
    "    trainset = torch.utils.data.Subset(train_val_dataset, train_indices)\n",
    "    valset = torch.utils.data.Subset(train_val_dataset, val_indices)\n",
    "\n",
    "    return trainset,valset,testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83bb72f8-7200-4a9e-ba8a-d1644ce63e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset, valset, testset = get_dataset(Path(dataset_path) / 'data', val_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b47ff9f2-a618-42e7-9aa1-6494c3770eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(tensor):\n",
    "    probabilities = torch.softmax(tensor, dim=1)\n",
    "    return -torch.sum(probabilities * torch.log2(probabilities + 1e-10), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d54100a7-426f-4c7e-8a85-b67245ec923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(trainset,valset, model, criterion, optimizer, num_epochs, bs):\n",
    "    dataloaders_strong = {\n",
    "        'train': data.DataLoader(trainset, batch_size=bs, shuffle=True),\n",
    "        'val': data.DataLoader(valset, batch_size=bs, shuffle=True)\n",
    "    }\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, epochs=num_epoch, steps_per_epoch=len(dataloaders_strong['train']), max_lr=0.1)\n",
    "    dataset_sizes_strong = {'train': len(trainset), 'val': len(valset)}\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    epochs_without_improvement = 0  # Reset the counter since there is improvement\n",
    "\n",
    "    # Lists to store entropy values for each epoch\n",
    "    entropy_values_train = []\n",
    "    entropy_values_val = []\n",
    "    # Lists to store loss of each epoch\n",
    "    E_loss_train = []\n",
    "    E_loss_val = []\n",
    "    E_accuracy_train = []\n",
    "    E_accuracy_val = []\n",
    "    # Lists to store balanced accuracy for training and validation phases\n",
    "    balanced_acc_train = []\n",
    "    balanced_acc_val = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        c = 0\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            all_labels = []\n",
    "            all_preds = []\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                        \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            epoch_entropy = 0.0  # Variable to store epoch entropy\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in tqdm(dataloaders_strong[phase], \"Running \" + phase):\n",
    "                inputs = inputs.to(device)                \n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    # Calculate balanced accuracy only if all unique predicted classes are present in true labels\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "                        balanced_acc = balanced_accuracy_score(labels.cpu(), preds.cpu())\n",
    "                        \n",
    "                    all_labels.extend(labels.cpu().tolist())\n",
    "                    all_preds.extend(preds.cpu().tolist())\n",
    "\n",
    "                    # Backpropagation and optimization\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()   \n",
    "                        scheduler.step()               \n",
    "                    \n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                # Calculate entropy for each batch and accumulate for epoch entropy\n",
    "                e = entropy(outputs)\n",
    "                epoch_entropy += torch.sum(e)\n",
    "\n",
    "            \n",
    "            if phase == 'train':\n",
    "                epoch_loss = running_loss / dataset_sizes_strong[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes_strong[phase]\n",
    "                epoch_entropy /= dataset_sizes_strong[phase] # Calculate average epoch entropy\n",
    "            else:\n",
    "                epoch_loss = running_loss / (dataset_sizes_strong[phase]-c)\n",
    "                epoch_acc = running_corrects.double() / (dataset_sizes_strong[phase]-c)\n",
    "                epoch_entropy /= (dataset_sizes_strong[phase]-c) # Calculate average epoch entropy\n",
    "\n",
    "                \n",
    "                # print('Number of skipped:', c)\n",
    "                # print()\n",
    "            \n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            print('{} Epoch Entropy: {:.4f}'.format(phase, epoch_entropy.item()))\n",
    "            print('{} Balanced Accuracy: {:.4f}'.format(phase, balanced_acc))\n",
    "            print()\n",
    "\n",
    "            if phase == 'train':\n",
    "                entropy_values_train.append(epoch_entropy.item())\n",
    "                E_loss_train.append(epoch_loss)\n",
    "                balanced_acc_train.append(balanced_acc)  # Append balanced accuracy for training\n",
    "                E_accuracy_train.append(epoch_acc)\n",
    "            else:\n",
    "                entropy_values_val.append(epoch_entropy.item())\n",
    "                E_loss_val.append(epoch_loss)\n",
    "                balanced_acc_val.append(balanced_acc)  # Append balanced accuracy for validation\n",
    "                E_accuracy_val.append(epoch_acc)\n",
    "            \n",
    "            # Confusion Matrix\n",
    "            # cm = confusion_matrix(all_labels, all_preds)\n",
    "            # disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "            # disp.plot()\n",
    "            # plt.figure(figsize=(8, 6))\n",
    "            # plt.show()\n",
    "            \n",
    "            # Deep copy the model if the validation accuracy improves\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                best_epoch_val = epoch\n",
    "                \n",
    "                \n",
    "            # Early stopping criteria\n",
    "            if phase == 'val' and epoch > 0:\n",
    "                if epoch_loss <= 1e-7:\n",
    "                    print('Loss reached 0...')\n",
    "                    return (\n",
    "                        model,\n",
    "                        entropy_values_train,\n",
    "                        entropy_values_val,\n",
    "                        E_loss_train,\n",
    "                        E_loss_val,\n",
    "                        balanced_acc_train,\n",
    "                        balanced_acc_val\n",
    "                    ) \n",
    "                    \n",
    "                if epoch_loss >= E_loss_val[-1]:\n",
    "                    epochs_without_improvement += 1\n",
    "                else:\n",
    "                    epochs_without_improvement = 0\n",
    "\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print('Early stopping due to no improvement in validation loss.')\n",
    "                    return (\n",
    "                        model,\n",
    "                        entropy_values_train,\n",
    "                        entropy_values_val,\n",
    "                        E_loss_train,\n",
    "                        E_loss_val,\n",
    "                        balanced_acc_train,\n",
    "                        balanced_acc_val\n",
    "                    ) \n",
    "                \n",
    "            # Overfitting criteria\n",
    "            if phase == 'train' and epoch_loss <= E_loss_train[-1]:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print('Training stopped due to overfitting.')\n",
    "                    return (\n",
    "                        model,\n",
    "                        entropy_values_train,\n",
    "                        entropy_values_val,\n",
    "                        E_loss_train,\n",
    "                        E_loss_val,\n",
    "                        balanced_acc_train,\n",
    "                        balanced_acc_val\n",
    "                    )\n",
    "            else:\n",
    "                epochs_without_improvement = 0\n",
    "                \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    print('Best Val Acc. was achieved at epoch', best_epoch_val)\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return (\n",
    "        model,\n",
    "        entropy_values_train,\n",
    "        entropy_values_val,\n",
    "        E_loss_train,\n",
    "        E_loss_val,\n",
    "        balanced_acc_train,\n",
    "        balanced_acc_val,\n",
    "        E_accuracy_train,\n",
    "        E_accuracy_val\n",
    "    )\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66509907-64e6-4e31-a287-5c6dd90837d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, testset, confidence_threshold):\n",
    "    running_corrects = 0\n",
    "    testloader = data.DataLoader(testset, batch_size=bs, shuffle=True)\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    c = 0\n",
    "    confidence_scores_distribution = []\n",
    "    model.eval()\n",
    "    \n",
    "    for inputs, labels in tqdm(testloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # Calculate confidence score for each sample in the batch\n",
    "            confidence_scores = 1 - entropy(outputs)\n",
    "            if confidence_threshold == 0:\n",
    "                confidence_scores_distribution.append(confidence_scores.cpu().numpy())\n",
    "\n",
    "            # Skip predictions if confidence score is greater than the threshold\n",
    "            skip_mask = confidence_scores >= confidence_threshold\n",
    "                    \n",
    "            c += len(skip_mask) - skip_mask.sum().item()\n",
    "            inputs = inputs[skip_mask]\n",
    "            labels = labels[skip_mask]\n",
    "            preds = preds[skip_mask]\n",
    "                \n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "\n",
    "    test_accuracy = running_corrects / (len(testset)-c)\n",
    "    balanced_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    print(\"The Test Accuracy:\", test_accuracy)\n",
    "    print(\"The Test Balanced Accuracy:\", balanced_acc)\n",
    "    print(\"Skipped Inputs:\", c)\n",
    "    if confidence_threshold == 0:\n",
    "        confidence_scores_distribution = np.concatenate(confidence_scores_distribution)\n",
    "        # plt.hist(confidence_scores_distribution, bins=100)\n",
    "        # plt.gca().set(title='Confidence Histogram', ylabel='Occurrences');\n",
    "        # plt.show()\n",
    "        \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=classes )\n",
    "    disp.plot()\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.show()\n",
    "    return balanced_acc, 1 - c / len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47d8553c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, requires_grad=True):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c7968e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, use_pretrained=True):\n",
    "    model_ft = None\n",
    "    input_size = 256\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        # Resnet50\n",
    "        model_ft = models.resnet50(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, requires_grad=True)\n",
    "        model_ft.fc = nn.Linear(model_ft.fc.in_features, num_classes)\n",
    "        \n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        # VGG16_bn\n",
    "        model_ft = models.vgg16_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, requires_grad=True)\n",
    "        model_ft.classifier[6] = nn.Linear(4096, num_classes)\n",
    "\n",
    "    elif model_name == \"googlenet\":\n",
    "        # GoogLeNet\n",
    "        model_ft = torchvision.models.googlenet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, requires_grad=True)\n",
    "        model_ft.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        # Densenet\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, requires_grad=True)\n",
    "        model_ft.classifier = nn.Linear(1024, num_classes)\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ed233be",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "584f8883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution_by_class(labels, classes):\n",
    "\n",
    "    labels, count = np.unique(np.asarray(labels), return_counts=True)\n",
    "    labels = np.asarray([classes[int(i)] for i in labels])    \n",
    "    \n",
    "    plt.figure(figsize=(10, 6))  # Set the size of the figure\n",
    "    \n",
    "    x = np.arange(len(classes))  # Generate an array of class indices\n",
    "    plt.bar(x, count)\n",
    "    plt.xticks(x, classes)  # Set custom x-axis tick positions and labels\n",
    "    plt.xlabel(\"Classes\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Class Count\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db923780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 134748 val : 33687 test : 29724\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Separate the train, val, and test sets\n",
    "trainset,valset,testset = get_dataset(dataset_path, val_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5997bcf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAncAAAGDCAYAAABJITbwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAAsTAAALEwEAmpwYAAAhYElEQVR4nO3de7BfZX3v8ffHIIpVASWlNKEENdUiVqo5glo91igEtQZbL6Aj0UPlOGK1aq1QHXFUevCo9VagQyUFWg8R8YYajSmo0FaQoNzVsrlJMlyi4VLFW/B7/vg9W35udpKdy94/8vB+zfxmr/V9nrXWs5hhzSfrmqpCkiRJfXjAqAcgSZKkbcdwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw52k+50k70ryr6MehyRNB8OdpC4leXmSVUl+nOSmJF9O8scjGkuSvCHJFUl+kmR1kk8lecI0b3dekkqyw3RuR9J9i+FOUneSvBn4MPB3wO7A7wEnAotHNKSPAG8E3gA8Avh94HPA80c0HkkdM9xJ6kqSnYF3A0dV1Weq6idV9cuq+kJVvXUDy3wqyc1J7khyXpLHD7U9L8lVSf47yZokf93quyX5YpLbk6xLcn6Sex1Tk8wHjgIOq6pzq+rnVXVXVX2iqo4fH3OS05OsTXJDkneMr2viJeSJZ+OSfD3Je5L8RxvjV5Ps1rqf1/7e3s5gPnVr//tKuu8z3EnqzVOBBwOf3YxlvgzMB34b+DbwiaG2U4D/XVUPA/YFzm31twCrgdkMzg7+LTDZ9xwXAqur6lsb2f7HgJ2BRwH/EzgcePVmjP/lrf9vAzsCf93qz2x/d6mqh1bVNzdjnZK2U4Y7Sb15JPDDqlo/1QWqamlV/XdV/Rx4F/DEdgYQ4JfAPkkeXlW3VdW3h+p7AHu1M4Pn1+Qf634kcNOGtp1kFnAocEwbw/XAB4FXTnX8wD9X1X9V1U+BM4H9NmNZSZ0x3EnqzY+A3ab6EEGSWUmOT3JNkjuB61vT+KXNPweeB9yQ5BtDlzbfD4wBX01ybZKjNzKePTYyhN2ABwI3DNVuAOZMZfzNzUPTdwEP3YxlJXXGcCepN98Efg4cMsX+L2fwoMVzGFwandfqAaiqi6pqMYNLnp9jcGaMdpbtLVX1KOCFwJuTLJxk/ecAc5Ms2MD2f8jgLOBeQ7XfA9a06Z8ADxlq+50p7hdMfplYUucMd5K6UlV3AO8ETkhySJKHJHlgkoOT/N9JFnkYgzD4IwYh6u/GG5LsmOQVSXauql8CdwK/am0vSPKYJAHuAO4eb5swnqsZPKl7RpJntXU+OMmhSY6uqrsZBMbjkjwsyV7Am4HxhyguAZ6Z5PfapeJjNuM/x9o2pkdtxjKStnOGO0ndqaoPMghI72AQcG4EXs/gzNtEpzO4DLoGuAq4YEL7K4Hr2yXb1wKvaPX5wL8BP2ZwtvDEqvraBob0BuAfgBOA24FrgBcBX2jtf8ngDN21wL8D/w9Y2vZlJfBJ4DLgYuCLm/wP0FTVXcBxwH+0p3oPmOqykrZfmfz+X0mSJG2PPHMnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1JEpvcF9SyRZCrwAuLWq9p3Q9hbgA8Dsqvphe0/URxi8Bf4u4FXjn/hJsoTB6wwA3ltVp7X6k4FTgZ2A5cAbq6qSPILBawPmMXjT/Eur6rZNjXe33XarefPmbc0uS5IkzYiLL774h1U1e7K2aQt3DILXPzB4h9SvJdkTOBD4wVD5YAbvjJoP7A+cBOzfgtqxwAIGb1q/OMnZLaydBLwGuJBBuFvE4OPfRwPnVNXx7XNARwNv29Rg582bx6pVq7Z4ZyVJkmZKkhs21DZtl2Wr6jxg3SRNHwL+ht/8LM5i4PQauADYJckewEHAyqpa1wLdSmBRa3t4VV3QPtR9Ovd8amgxcFqbPo2pf4JIkiRpuzej99wlWQysqapLJzTNYfAG+XGrW21j9dWT1AF2r6qb2vTNwO4bGc+RSVYlWbV27drN3R1JkqT7nBkLd0keAvwtg28+zoh2Vm+Dn+CoqpOrakFVLZg9e9LL1pIkSduVmTxz92hgb+DSJNcDc4FvJ/kdBt903HOo79xW21h97iR1gFvaZVva31u3+Z5IkiTdR81YuKuqy6vqt6tqXlXNY3Ap9UlVdTNwNnB4Bg4A7miXVlcABybZNcmuDB7EWNHa7kxyQHvS9nDg821TZwNL2vSSobokSVL3pi3cJTkD+Cbw2CSrkxyxke7LgWuBMeCfgNcBVNU64D3ARe337laj9fl4W+YaBk/KAhwPPDfJ1cBz2rwkSdL9Qga3pWnBggXlq1AkSdL2IMnFVbVgsja/UCFJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSR3YY9QDuT+Yd/aVRD0G637v++OePegjTyuOMNHqjPs545k6SJKkjhjtJkqSOGO4kSZI6YriTJEnqiOFOkiSpI4Y7SZKkjhjuJEmSOmK4kyRJ6ojhTpIkqSOGO0mSpI4Y7iRJkjpiuJMkSeqI4U6SJKkjhjtJkqSOGO4kSZI6YriTJEnqiOFOkiSpI4Y7SZKkjhjuJEmSOmK4kyRJ6ojhTpIkqSOGO0mSpI4Y7iRJkjpiuJMkSeqI4U6SJKkjhjtJkqSOGO4kSZI6YriTJEnqiOFOkiSpI4Y7SZKkjkxbuEuyNMmtSa4Yqr0/yfeSXJbks0l2GWo7JslYku8nOWiovqjVxpIcPVTfO8mFrf7JJDu2+oPa/Fhrnzdd+yhJknRfM51n7k4FFk2orQT2rao/BP4LOAYgyT7AocDj2zInJpmVZBZwAnAwsA9wWOsL8D7gQ1X1GOA24IhWPwK4rdU/1PpJkiTdL0xbuKuq84B1E2pfrar1bfYCYG6bXgwsq6qfV9V1wBjwlPYbq6prq+oXwDJgcZIAzwbOasufBhwytK7T2vRZwMLWX5IkqXujvOfufwFfbtNzgBuH2la32obqjwRuHwqK4/XfWFdrv6P1v5ckRyZZlWTV2rVrt3qHJEmSRm0k4S7J24H1wCdGsf1xVXVyVS2oqgWzZ88e5VAkSZK2iR1meoNJXgW8AFhYVdXKa4A9h7rNbTU2UP8RsEuSHdrZueH+4+tanWQHYOfWX5IkqXszeuYuySLgb4AXVtVdQ01nA4e2J133BuYD3wIuAua3J2N3ZPDQxdktFH4NeHFbfgnw+aF1LWnTLwbOHQqRkiRJXZu2M3dJzgCeBeyWZDVwLIOnYx8ErGzPOFxQVa+tqiuTnAlcxeBy7VFVdXdbz+uBFcAsYGlVXdk28TZgWZL3At8BTmn1U4B/STLG4IGOQ6drHyVJku5rpi3cVdVhk5RPmaQ23v844LhJ6suB5ZPUr2XwNO3E+s+Al2zWYCVJkjrhFyokSZI6YriTJEnqiOFOkiSpI4Y7SZKkjhjuJEmSOmK4kyRJ6ojhTpIkqSOGO0mSpI4Y7iRJkjpiuJMkSeqI4U6SJKkjhjtJkqSOGO4kSZI6YriTJEnqiOFOkiSpI4Y7SZKkjhjuJEmSOmK4kyRJ6ojhTpIkqSOGO0mSpI4Y7iRJkjpiuJMkSeqI4U6SJKkjhjtJkqSOGO4kSZI6YriTJEnqiOFOkiSpI4Y7SZKkjhjuJEmSOmK4kyRJ6ojhTpIkqSOGO0mSpI4Y7iRJkjpiuJMkSeqI4U6SJKkjhjtJkqSOGO4kSZI6Mm3hLsnSJLcmuWKo9ogkK5Nc3f7u2upJ8tEkY0kuS/KkoWWWtP5XJ1kyVH9yksvbMh9Nko1tQ5Ik6f5gOs/cnQosmlA7GjinquYD57R5gIOB+e13JHASDIIacCywP/AU4NihsHYS8Jqh5RZtYhuSJEndm7ZwV1XnAesmlBcDp7Xp04BDhuqn18AFwC5J9gAOAlZW1bqqug1YCSxqbQ+vqguqqoDTJ6xrsm1IkiR1b6bvudu9qm5q0zcDu7fpOcCNQ/1Wt9rG6qsnqW9sG/eS5Mgkq5KsWrt27RbsjiRJ0n3LyB6oaGfcapTbqKqTq2pBVS2YPXv2dA5FkiRpRsx0uLulXVKl/b211dcAew71m9tqG6vPnaS+sW1IkiR1b6bD3dnA+BOvS4DPD9UPb0/NHgDc0S6trgAOTLJre5DiQGBFa7szyQHtKdnDJ6xrsm1IkiR1b4fpWnGSM4BnAbslWc3gqdfjgTOTHAHcALy0dV8OPA8YA+4CXg1QVeuSvAe4qPV7d1WNP6TxOgZP5O4EfLn92Mg2JEmSujdt4a6qDttA08JJ+hZw1AbWsxRYOkl9FbDvJPUfTbYNSZKk+wO/UCFJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktSRkYS7JG9KcmWSK5KckeTBSfZOcmGSsSSfTLJj6/ugNj/W2ucNreeYVv9+koOG6otabSzJ0SPYRUmSpJGY8XCXZA7wBmBBVe0LzAIOBd4HfKiqHgPcBhzRFjkCuK3VP9T6kWSfttzjgUXAiUlmJZkFnAAcDOwDHNb6SpIkdW9Ul2V3AHZKsgPwEOAm4NnAWa39NOCQNr24zdPaFyZJqy+rqp9X1XXAGPCU9hurqmur6hfAstZXkiSpezMe7qpqDfAB4AcMQt0dwMXA7VW1vnVbDcxp03OAG9uy61v/Rw7XJyyzofq9JDkyyaokq9auXbv1OydJkjRio7gsuyuDM2l7A78L/BaDy6ozrqpOrqoFVbVg9uzZoxiCJEnSNjWKy7LPAa6rqrVV9UvgM8DTgV3aZVqAucCaNr0G2BOgte8M/Gi4PmGZDdUlSZK6N4pw9wPggCQPaffOLQSuAr4GvLj1WQJ8vk2f3eZp7edWVbX6oe1p2r2B+cC3gIuA+e3p2x0ZPHRx9gzslyRJ0sjtsOku21ZVXZjkLODbwHrgO8DJwJeAZUne22qntEVOAf4lyRiwjkFYo6quTHImg2C4Hjiqqu4GSPJ6YAWDJ3GXVtWVM7V/kiRJozTj4Q6gqo4Fjp1QvpbBk64T+/4MeMkG1nMccNwk9eXA8q0fqSRJ0vbFL1RIkiR1xHAnSZLUEcOdJElSR6YU7pI8fSo1SZIkjdZUz9x9bIo1SZIkjdBGn5ZN8lTgacDsJG8eano4g9eMSJIk6T5kU69C2RF4aOv3sKH6ndzzwmFJkiTdR2w03FXVN4BvJDm1qm6YoTFJkiRpC031JcYPSnIyMG94map69nQMSpIkSVtmquHuU8A/Ah8H7p6+4UiSJGlrTDXcra+qk6Z1JJIkSdpqU30VyheSvC7JHkkeMf6b1pFJkiRps031zN2S9vetQ7UCHrVthyNJkqStMaVwV1V7T/dAJEmStPWmFO6SHD5ZvapO37bDkSRJ0taY6mXZ/zE0/WBgIfBtwHAnSZJ0HzLVy7J/OTyfZBdg2XQMSJIkSVtuqk/LTvQTwPvwJEmS7mOmes/dFxg8HQswC/gD4MzpGpQkSZK2zFTvufvA0PR64IaqWj0N45EkSdJWmNJl2ar6BvA94GHArsAvpnNQkiRJ2jJTCndJXgp8C3gJ8FLgwiQvns6BSZIkafNN9bLs24H/UVW3AiSZDfwbcNZ0DUySJEmbb6pPyz5gPNg1P9qMZSVJkjRDpnrm7itJVgBntPmXAcunZ0iSJEnaUhsNd0keA+xeVW9N8mfAH7embwKfmO7BSZIkafNs6szdh4FjAKrqM8BnAJI8obX96TSOTZIkSZtpU/fN7V5Vl08sttq8aRmRJEmSttimwt0uG2nbaRuOQ5IkSdvApsLdqiSvmVhM8hfAxdMzJEmSJG2pTd1z91fAZ5O8gnvC3AJgR+BF0zguSZIkbYGNhruqugV4WpI/AfZt5S9V1bnTPjJJkiRttim9566qvgZ8bZrHIkmSpK3kVyYkSZI6YriTJEnqiOFOkiSpI4Y7SZKkjowk3CXZJclZSb6X5LtJnprkEUlWJrm6/d219U2SjyYZS3JZkicNrWdJ6391kiVD9Scnubwt89EkGcV+SpIkzbRRnbn7CPCVqnoc8ETgu8DRwDlVNR84p80DHAzMb78jgZMAkjwCOBbYH3gKcOx4IGx9XjO03KIZ2CdJkqSRm/Fwl2Rn4JnAKQBV9Yuquh1YDJzWup0GHNKmFwOn18AFwC5J9gAOAlZW1bqqug1YCSxqbQ+vqguqqoDTh9YlSZLUtVGcudsbWAv8c5LvJPl4kt8Cdq+qm1qfm4Hd2/Qc4Mah5Ve32sbqqyep30uSI5OsSrJq7dq1W7lbkiRJozeKcLcD8CTgpKr6I+An3HMJFoB2xq2meyBVdXJVLaiqBbNnz57uzUmSJE27UYS71cDqqrqwzZ/FIOzd0i6p0v7e2trXAHsOLT+31TZWnztJXZIkqXszHu6q6mbgxiSPbaWFwFXA2cD4E69LgM+36bOBw9tTswcAd7TLtyuAA5Ps2h6kOBBY0druTHJAe0r28KF1SZIkdW1K35adBn8JfCLJjsC1wKsZBM0zkxwB3AC8tPVdDjwPGAPuan2pqnVJ3gNc1Pq9u6rWtenXAacCOwFfbj9JkqTujSTcVdUlwIJJmhZO0reAozawnqXA0knqq4B9t26UkiRJ2x+/UCFJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktSRkYW7JLOSfCfJF9v83kkuTDKW5JNJdmz1B7X5sdY+b2gdx7T695McNFRf1GpjSY6e8Z2TJEkakVGeuXsj8N2h+fcBH6qqxwC3AUe0+hHAba3+odaPJPsAhwKPBxYBJ7bAOAs4ATgY2Ac4rPWVJEnq3kjCXZK5wPOBj7f5AM8GzmpdTgMOadOL2zytfWHrvxhYVlU/r6rrgDHgKe03VlXXVtUvgGWtryRJUvdGdebuw8DfAL9q848Ebq+q9W1+NTCnTc8BbgRo7Xe0/r+uT1hmQ3VJkqTuzXi4S/IC4Naqunimtz3JWI5MsirJqrVr1456OJIkSVttFGfung68MMn1DC6ZPhv4CLBLkh1an7nAmja9BtgToLXvDPxouD5hmQ3V76WqTq6qBVW1YPbs2Vu/Z5IkSSM24+Guqo6pqrlVNY/BAxHnVtUrgK8BL27dlgCfb9Nnt3la+7lVVa1+aHuadm9gPvAt4CJgfnv6dse2jbNnYNckSZJGbodNd5kxbwOWJXkv8B3glFY/BfiXJGPAOgZhjaq6MsmZwFXAeuCoqrobIMnrgRXALGBpVV05o3siSZI0IiMNd1X1deDrbfpaBk+6TuzzM+AlG1j+OOC4SerLgeXbcKiSJEnbBb9QIUmS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1JEZD3dJ9kzytSRXJbkyyRtb/RFJVia5uv3dtdWT5KNJxpJcluRJQ+ta0vpfnWTJUP3JSS5vy3w0SWZ6PyVJkkZhFGfu1gNvqap9gAOAo5LsAxwNnFNV84Fz2jzAwcD89jsSOAkGYRA4FtgfeApw7HggbH1eM7TcohnYL0mSpJGb8XBXVTdV1bfb9H8D3wXmAIuB01q304BD2vRi4PQauADYJckewEHAyqpaV1W3ASuBRa3t4VV1QVUVcPrQuiRJkro20nvukswD/gi4ENi9qm5qTTcDu7fpOcCNQ4utbrWN1VdPUpckSereyMJdkocCnwb+qqruHG5rZ9xqBsZwZJJVSVatXbt2ujcnSZI07UYS7pI8kEGw+0RVfaaVb2mXVGl/b231NcCeQ4vPbbWN1edOUr+Xqjq5qhZU1YLZs2dv3U5JkiTdB4ziadkApwDfraq/H2o6Gxh/4nUJ8Pmh+uHtqdkDgDva5dsVwIFJdm0PUhwIrGhtdyY5oG3r8KF1SZIkdW2HEWzz6cArgcuTXNJqfwscD5yZ5AjgBuClrW058DxgDLgLeDVAVa1L8h7gotbv3VW1rk2/DjgV2An4cvtJkiR1b8bDXVX9O7Ch984tnKR/AUdtYF1LgaWT1FcB+27FMCVJkrZLfqFCkiSpI4Y7SZKkjhjuJEmSOmK4kyRJ6ojhTpIkqSOGO0mSpI4Y7iRJkjpiuJMkSeqI4U6SJKkjhjtJkqSOGO4kSZI6YriTJEnqiOFOkiSpI4Y7SZKkjhjuJEmSOmK4kyRJ6ojhTpIkqSOGO0mSpI4Y7iRJkjpiuJMkSeqI4U6SJKkjhjtJkqSOGO4kSZI6YriTJEnqiOFOkiSpI4Y7SZKkjhjuJEmSOmK4kyRJ6ojhTpIkqSOGO0mSpI4Y7iRJkjpiuJMkSeqI4U6SJKkjhjtJkqSOGO4kSZI6YriTJEnqiOFOkiSpI92GuySLknw/yViSo0c9HkmSpJnQZbhLMgs4ATgY2Ac4LMk+ox2VJEnS9Osy3AFPAcaq6tqq+gWwDFg84jFJkiRNu17D3RzgxqH51a0mSZLUtR1GPYBRSnIkcGSb/XGS749yPNou7Ab8cNSD0JbL+0Y9AmmTPM5s52boOLPXhhp6DXdrgD2H5ue22m+oqpOBk2dqUNr+JVlVVQtGPQ5J/fI4o63V62XZi4D5SfZOsiNwKHD2iMckSZI07bo8c1dV65O8HlgBzAKWVtWVIx6WJEnStOsy3AFU1XJg+ajHoe54GV/SdPM4o62Sqhr1GCRJkrSN9HrPnSRJ0v2S4U7bpSQ/bn/nJflpkkuSXJXk9CQPbG3PSnJHa/tekg8MLf+qJGtb2/hvn7a+KybZ3qlJrhvq+5+t7+okD5jQ95Ik+7fpv0rysyQ7D7U/K0kl+dOh2hdb/bNt+bGhsV+S5Gnb/r+ipM2V5O72/+QVSb6QZJdWHz4Wjf8Ob23XJ9ltwno2dAy6NsljJ/T9cJK3ten92vFj0YQ+leSDQ/N/neRdSd4+tP67h6bfME3/iXQfYLhTD66pqv2AJzB47c1Lh9rOb21/BLwgydOH2j5ZVfsN/a7axHbeOtT3aVV1PfAD4BnjHZI8DnhYVV3YSocxeHr7zyasazXw9okbqKoXtfH+xfjY2+8/NzE2STPjp+3/yX2BdcBRQ23XTDimnL6JdU12DFrG4A0PALR/PL641WFwTPn39nfYz4E/mxgiq+q48fUPjX2/qvroZu63tiOGO3Wjqu4GvsUkXyOpqp8Cl0zWtpXOYOhA3KaXASR5NPBQ4B3c+0B8KXBHkudu4/FImjnfZHqOKS8bmn8mcENV3ZAkwEuAVwHPTfLgoX7rGTyI8aZtPB5thwx36kY70O0PfGWStl2B+cB5Q+WXTbgkstMmNvH+ob6faLUzgUOSjD95/jIGB2e4J+idDzw2ye4T1nccg+AnaTuTZBawkN98h+qjJxxTnrGBxcfd6xhUVZcDv0ryxNbnUO45pjwNuK6qrgG+Djx/wvpOAF4xfBuI7p8Md+rBo5NcAtwC3FRVlw21PSPJpQy+ULKiqm4eapt4SeSnm9jO8GXZVwBU1S3AFcDCJPsB66tq/J69w4BlVfUr4NMM/sX9a1V1HkCSP96SnZY0Eju1483NwO7AyqG2iZdlz9/EujZ0DDoDOLT9o/EQ4FOtfhj3XJ5dxoQrAlV1J3A64P1093OGO/Vg/J67RwNPTvLCobbzq+qJwOOBI1oA29bGL83++l/YSZ7A4EzhyiTXt7aJl2bBs3fS9uan7XizFxB+8567bWUZg3uHnwNcVlW3tDOFfw68sx1TPgYsSvKwCct+GDgC+K1pGJe2E4Y7daOqfggcDRwzSdt1wPHA26Zh058BnsfgkuzwTc/vqqp57fe7wO8m+Y0PPVfVV4FdgT+chnFJmiZVdReDM2RvGbotY1ut+xrghwyOWeOXZBcyCHp7tmPKXgyuCLxowrLrGNwucsS2HJO2L4Y79eZzwEM2cK/LPwLPTDKvzU+832X8dSOPba84Gf+NX059/4T+OwJU1e0Mbqy+paqubX0PBT47Yfuf5Tcfvhh3HLDnFuyrpBGqqu8Al3HPWfmJ99wNXx69bOiY8vettqFjEAxC3eMY/OORto2Jx5RPM/kVgQ8Cu01S1/2EX6iQJEnqiGfuJEmSOmK4kyRJ6ojhTpIkqSOGO0mSpI4Y7iRJkjpiuJOkDUjyO0mWJbkmycVJlif5/SRXbHppSRqNbfriRUnqRftI+2eB06rq0FZ7IoNPTknSfZZn7iRpcn8C/LKq/nG8UFWXAjeOzyeZl+T8JN9uv6e1+h5Jzmsvpr0iyTOSzEpyapu/PMmbWt9HJ/lKOzN4fpLHtfpLWt9Lk5w3s7suaXvmmTtJmty+wMWb6HMr8Nyq+lmS+Qy+KrAAeDmwoqqOa98EfQiwHzCnqvYFSLJLW8fJwGur6uok+wMnAs8G3gkcVFVrhvpK0iYZ7iRpyz0Q+Ick+wF3A7/f6hcBS5M8EPhcVV2S5FrgUUk+BnwJ+GqShwJPAz41uAoMwIPa3/8ATk1yJvd8gkqSNsnLspI0uSuBJ2+iz5uAW4AnMjhjN/694fOAZwJrGAS0w6vqttbv68BrgY8zOAbfXlX7Df3+oK3jtcA7GHx3+OIkj9zG+yepU4Y7SZrcucCDkhw5XkjyhwzC1ridgZuq6lfAK4FZrd9ewC1V9U8MQtyTkuwGPKCqPs0gtD2pqu4ErkvykrZc2kMbJHl0VV1YVe8E1k7YriRtkOFOkiZRVQW8CHhOexXKlcD/AW4e6nYisCTJpcDjgJ+0+rOAS5N8B3gZ8BFgDvD1JJcA/woc0/q+AjiireNKYHGrv789eHEF8J/ApdOyo5K6k8HxS5IkST3wzJ0kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1JH/D0asWomvBuvCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_ = trainset.subset.dataset.dataset.dataset\n",
    "labels = np.asarray(data_.imgs)[trainset.subset.dataset.dataset.indices]\n",
    "labels = labels[trainset.subset.dataset.indices]\n",
    "labels = labels[trainset.subset.indices, 1]\n",
    "\n",
    "plot_distribution_by_class(labels, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c50fe9-b336-4f34-9de3-fa5c8c4c5d95",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model_name(model_name):\n",
    "    print(model_name)\n",
    "    # Initialize the model\n",
    "    model, input_size = initialize_model(model_name, num_classes, use_pretrained=True)\n",
    "    \n",
    "    \n",
    "    # Send model to GPU, if available\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer = optim.SGD(model.parameters())\n",
    "    model, entropy_values_train, entropy_values_val, E_loss_train, E_loss_val, balanced_acc_train, balanced_acc_val,E_accuracy_train,E_accuracy_val = train_model(\n",
    "        trainset,valset,model, criterion, optimizer, num_epoch, bs)\n",
    "\n",
    "        \n",
    "    # Plot entropy values\n",
    "    plt.figure()\n",
    "    plt.plot(entropy_values_train, label='Train')\n",
    "    plt.plot(entropy_values_val, label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Entropy')\n",
    "    plt.title('Entropy Trend')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot loss values\n",
    "    plt.figure()\n",
    "    plt.plot(E_loss_train, label='Train')\n",
    "    plt.plot(E_loss_val, label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Trend')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot balanced loss values\n",
    "    plt.figure()\n",
    "    plt.plot(balanced_acc_train, label='Train')\n",
    "    plt.plot(balanced_acc_val, label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Balanced Accuracy')\n",
    "    plt.title('Balanced Accuracy Trend')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467e3b95",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_list = []\n",
    "model_list.append(train_model_name('densenet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28401d10-ca15-4ba4-b5e4-0b26fd48b6df",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_list.append(train_model_name('resnet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc590dbd-42f2-4cfb-a367-4baf0eda6f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list.append(train_model_name('googlenet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c31745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving models to file\n",
    "for i, model in enumerate(model_list):\n",
    "\n",
    "    # Define the file path to save the model\n",
    "    model_file1 = f'./model_state_{i}.pt'\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), model_file1)\n",
    "    \n",
    "    model_file2 = f'./model_entire_{i}.pt'\n",
    "    \n",
    "    # Save the entire model\n",
    "    torch.save(model, model_file2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a422cbe8-d193-4ab9-8a94-5fa9161b446b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confidence_thresholds = [0., .25, .5, .75, .9]\n",
    "model_names = {\n",
    "    0: \"DenseNet\",\n",
    "    1: \"ResNet\",\n",
    "    2: \"GoogleNet\"\n",
    "}\n",
    "\n",
    "confidence_results = [[] for _ in model_list]\n",
    "confidence_size = [[] for _ in model_list]\n",
    "for i, model in enumerate(model_list):\n",
    "    # # Load the model from file model\n",
    "    # model = torch.load(f'./model_entire_{i}.pt')\n",
    "    \n",
    "    # # Load the state dictionary from the model_state file\n",
    "    # state_dict = torch.load(f'./model_state_{i}.pt')\n",
    "    \n",
    "    # # Load the state dictionary into the model\n",
    "    # model.load_state_dict(state_dict)\n",
    "    \n",
    "    for confidence_threshold in confidence_thresholds:\n",
    "        print(f\"{model_names[i]} - Confidence {int(confidence_threshold * 100)}%\")\n",
    "        result, size = test_model(model, testset, confidence_threshold)\n",
    "        confidence_results[i].append(result)\n",
    "        confidence_size[i].append(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039ac8ca-1dc4-4aac-bee7-4f7887b677e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting confidence scores\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "fig.suptitle('Results by confidence thresholds')\n",
    "\n",
    "# plotting confidence\n",
    "for i, result in enumerate(confidence_results):\n",
    "    ax1.plot(confidence_thresholds, result, label=model_names[i])\n",
    "# plotting data size\n",
    "for i, size in enumerate(confidence_size):\n",
    "    ax2.plot(confidence_thresholds, size, label=model_names[i])\n",
    "\n",
    "# Set labels\n",
    "ax1.set_ylabel('Balanced Accuracy')\n",
    "# Display the legend\n",
    "ax1.legend()\n",
    "\n",
    "# Set labels\n",
    "ax2.set_xlabel('Confidence scores')\n",
    "ax2.set_ylabel('Percentage of retained dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6069569-e943-4364-9a1f-34ccecccd43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_to_numpy(dataloader):\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    for data, label in tqdm(dataloader):\n",
    "        # Reshape the data to 1D\n",
    "        data = data.view(-1)\n",
    "        data_list.append(data.numpy())\n",
    "        label_list.append(label)\n",
    "    return np.stack(data_list), np.asarray(label_list)\n",
    "\n",
    "# Convert dataloaders to numpy arrays\n",
    "print(\"Converting validation set\")\n",
    "valset.transform = transform_train\n",
    "val_X, val_Y = dataloader_to_numpy(valset)\n",
    "print(\"Converting train set\")\n",
    "train_X, train_Y = dataloader_to_numpy(trainset)\n",
    "print(\"Converting test set\")\n",
    "test_X, test_Y = dataloader_to_numpy(testset)\n",
    "\n",
    "# Concatenate training set and validation set\n",
    "train_X = np.concatenate((train_X, val_X), axis=0)\n",
    "train_Y = np.concatenate((train_Y, val_Y), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c611af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a dummy prediction, i.e Random Baseline\n",
    "# Create a dummy classifier with a strategy\n",
    "dummy_clf = DummyClassifier(strategy='uniform')\n",
    "\n",
    "# Train the dummy classifier\n",
    "dummy_clf.fit(train_X, train_Y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = dummy_clf.predict(test_X)\n",
    "\n",
    "# Evaluate the accuracy of the dummy classifier\n",
    "accuracy = dummy_clf.score(test_X, test_Y)\n",
    "\n",
    "print(\"Dummy Classifier Accuracy/ Random Baseline:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddac122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using SVM\n",
    "model_svc = SVC(decision_function_shape='ovr', C=100, kernel='rbf')\n",
    "model_svc.fit(train_X, train_Y)\n",
    "pred = model_svc.predict(test_X)\n",
    "acc = accuracy_score(test_Y, pred)\n",
    "print('Accuracy for SVM: ' + str(acc))\n",
    "balanced_svm = balanced_accuracy_score(test_Y, pred)  # Calculate balanced accuracy\n",
    "print(\"Balanced Accuracy:\"+ str(balanced_svm))\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_Y, pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=classes )\n",
    "disp.plot()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53f21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using MLP\n",
    "model_mlp = mlp(solver='lbfgs', alpha=1e-5, random_state=5, max_iter=100000)\n",
    "model_mlp.fit(train_X, train_Y)\n",
    "pred = model_mlp.predict(test_X)\n",
    "acc = accuracy_score(test_Y, pred)\n",
    "print('Accuracy for MLP: ' + str(acc))\n",
    "balanced_mlp = balanced_accuracy_score(test_Y, pred)  # Calculate balanced accuracy\n",
    "print(\"Balanced Accuracy:\"+ str(balanced_mlp))\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_Y, pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=classes )\n",
    "disp.plot()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
